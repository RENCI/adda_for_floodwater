#!/usr/bin/env python

# SPDX-FileCopyrightText: 2022 Renaissance Computing Institute. All rights reserved.
#
# SPDX-License-Identifier: GPL-3.0-or-later
# SPDX-License-Identifier: LicenseRef-RENCI
# SPDX-License-Identifier: MIT

#
# For each new source: Create a subclass that defines the functions:
# fetch_single_data(station)-> pd.DataFrame
# fetch_single_metadata(station,periods)-> pd.DataFrame
#
# OUTPUT product data are generated by 
#    1) Grab the rawe data from the selected data source
#    2) Build interpolation the data 
#    3) resample the interpolated data at the desired resample_rate
#        it is possible for these data to still contain NaNs
# excluded stations with insufficient data.
#

import traceback

import os,sys
import numpy as np
import pandas as pd
import xarray as xr
import datetime as dt
from utilities.utilities import utilities
import math

#Contrails
import urllib
import urllib.parse
import requests
from requests.exceptions import ConnectionError
from requests.exceptions import Timeout
from requests.exceptions import HTTPError
import xmltodict

# NOAA/NOS
import noaa_coops as coops

# NOAA/WEB API
from io import StringIO
import json

# NDBC # Continue to need siphon to get Lat/Lon values
from siphon.simplewebservice.ndbc import NDBC
#import buoypy as bp

# THREDDS (ADCIRC model)
import netCDF4 as nc4
import numpy as np
import numpy.ma as ma
import pandas as pd
from siphon.catalog import TDSCatalog
from collections import OrderedDict

GLOBAL_TIMEZONE='gmt' # Every source is set or presumed to return times in this zone

GLOBAL_FILL_VALUE='-99999'  # Always replaces final np.nans with this.

#UNITS='meters' # Now the code only applies to WL
def map_product_to_harvester_units(product):
    """
    The harvester dataset should be returning consistent metric units 
    for all sources. This dictionary performs the translation for supported 
    data products to known units This is mosty used for metadata construction

    Parameters:
        product: (str) The generic data product name (water_level, wind_speed, etc)

    Returns:
        units: (str) The chosen data unit for the specified product
    """
    product_unit_maps={
        'water_level': 'm',
        'predictions': 'm',
        'hourly_height': 'm',
        'wave_height': 'm',
        'river_water_level': 'm',
        'river_stream_elevation': 'm',
        'river_flow_volume': 'm^3ps',
        'coastal_water_level': 'm',
        'air_pressure':'mb',
        'wind_speed':'mps'
        }
    if product in product_unit_maps.keys():
        unit = product_unit_maps[product]
    else: 
        unit='NA'
    return unit

def replace_and_fill(df):
    """
    Replace all Nans with 'None" values with GLOBAL_FILL_VALUE
    """
    df=df.fillna(GLOBAL_FILL_VALUE)
    return df

def stations_resample(df, sample_mins=15)->pd.DataFrame:
    """
    Resample (all stations) on a pecified (eg 15min) basis. Setting sample_min
        to zero retains full resolution. Defaults to 15min

    NOTE: Final aggregated data still have flanked nans for some stations because
    The reported times might have been differently staggered. Often the case with river and
    buoy data 

    Parameters:
        df: A time series x stations dataframe
        sample_min. (Dafaut=15mins) A numerical value for the number of mins to resample
            setting to 0 disables any resampling and returns the raw data at full precision

    Returns:
        df_out. New time series every sample_mins x stations
    """
    if sample_mins==0:
        utilities.log.debug('resample freq set to 0. return all')
        return df
    timesample=f'{sample_mins}min'
    #utilities.log.info('Resampling freq set to {}'.format(timesample))
    dx=df.groupby(pd.Grouper(freq=timesample)).first().reset_index()
    return dx.set_index('TIME')

def stations_interpolate(df)->pd.DataFrame:
    """
    Interpolate each station column in the dataframe. 
    Final aggregated data still have flanked nans for some stations. 
    Defaults to: method='polynomial', order=1, limit=3, limit_direction= 'both'

    Parameters:
        df: A time series x stations data frame

    Returns:
        df_out. New time series x stations
    """
    utilities.log.info('Interpolating station data' )
    df.interpolate(method='polynomial', order=1, limit=3, inplace=True, limit_direction= 'both')
    return df

##
## Added a trap for RuntimeError. We have seen rare cases where the TDS call returns a DAP Failure. This return 
## Can takes minutes per station. So the normal eror trapping would continue to try all stations. Doing so for thie DAP
## Failure causes the jobs to run very long. So, now if we have any RuntimeErrors calling the TDS, we bail and move
## onto calling the next url in the list
## 
class fetch_station_data(object):
    """
    We expect upon entry to this class a list of station ids. The content of this list can
    vary depending on chosen data source. But usually this is simply a list of (str) station ids. If
    an ADCIRC Fort63_style lookup is requested, Then the elements of this list are tuples (stations,node)

    A list of times (periods) is required. The list contents can vary with data source. Generally, this is
    a list of one element; a tuple such as [(time1,time2)]. However, the caller could gang together multiple tuple ranges.
    For calling ADCIRC data, periods is a list of TDS URLs (which imply a range of times)

    The output data are aggregated station products into a dataframe: (TIME vs PRODUCT)
    with TIME as datetime timestamps and a column of data of the desired units and with a column
    name of the station

   Default return products wil be on the sampling_mins frequency
    """
    def __init__(self, stations, periods, resample_mins=15):
        """
        stations:  A list of stations <str> or <tuples> of (station,adcirc node) (str,int) 
        periods: A list of tuples. [(time1,time2)]
        """
        self._stations=stations
        self._periods=periods
        self._resampling_mins=resample_mins

    def interpolate_and_resample(self, dx, n_pad=0, sample_mins=15, int_limit=3)->pd.DataFrame:
        """
        An alternative way to interpolate and resample data. This approach generates
        an interpolated value at the sampling frequency. Tus precluding any implicit data time shifting
        that would occur using the simple resample methods

        Parameters:
            dx: DataFrame of read products including Nans
            n_pad: Additional trailng hours to add to the interpolation range 
            int_limit: Number of consecutive nans to interpolate over.
            sample_mins: the resampling in minutes

        Returns:
            Interpolated results: A dataframe (times x stations) for timerange  and input stations
        """
        try:
            if sample_mins>0:
                dformat='%Y-%m-%d %H' # Want string times back on-the-hour only
                timein = dt.datetime.strptime(min(dx.index).strftime(dformat), dformat)
                timeout = dt.datetime.strptime(max(dx.index+np.timedelta64(n_pad,'h')).strftime(dformat), dformat)
                # Generate the NEW augmented time range
                actualRange = dx.index
                normalRange = pd.date_range(str(timein), str(timeout), freq=f'{sample_mins*60.0}S') # This gets us the stepping we want
                datanormal=[x for x in normalRange if x not in actualRange] 
                # Assemble the union of values for the final data set. Exclude entries that already exist in the real data
                dappend = pd.concat([dx,pd.DataFrame(index=datanormal)],axis=0)
                dappend.sort_index(axis=0, ascending=True, inplace=True)
                df_smooth = dappend.interpolate(method='time',limit=int_limit, limit_direction= 'both')
                df_normal_smooth = df_smooth.loc[normalRange]
            else:
                df_normal_smooth = dx.interpolate(method='time',limit=int_limit, limit_direction= 'both') 
            df_normal_smooth.index.name='TIME'
        except Exception as ex:
            utilities.log.warn(f'Error Value: Failed interpolation {ex}: Probable empty station data')
            raise
        return df_normal_smooth
        #sys.exit(1)

    def old_interpolate_and_resample(self, dx, sample_mins=15)->pd.DataFrame:
        """
        The original approach used thro0ugh Jan 2023
        """
        dx_int = stations_interpolate(dx)
        df_resampled=stations_resample(dx_int, sample_mins)
        return df_resampled

    def aggregate_station_data(self, perform_interpolation=True)->pd.DataFrame:
        """
        Loop over the list of stations and fetch the products using the method of the relevant subclasses. Then concatenate them info single dataframe
        nans now get converted to the value in GLOBAL_FILL_VALUE

        Returns:
            Aggregated results: A dataframe (times x stations) for all periods and input stations
            perform_interpolation: bool. Generally True except if you want to fetch data from Harvester
        
        """
        use_new_interpolation=True  # False # This will go away after validation

        aggregateData = list()
        excludedStations=list()
        template = "An exception of type {0} occurred. Arguments:\n{1!r}"

        for station in self._stations:
            #utilities.log.info(station)    
            try:
                dx = self.fetch_single_product(station, self._periods)
                try:
                    if use_new_interpolation:
                        if perform_interpolation:
                            df_int = self.interpolate_and_resample(dx, n_pad=0, sample_mins=self._resampling_mins, int_limit=2)
                            df_int = stations_resample(df_int,sample_mins=self._resampling_mins)
                        else:
                            df_int = stations_resample(dx,sample_mins=self._resampling_mins)
                        aggregateData.append(df_int)
                    else:
                        df_int = self.old_interpolate_and_resample(dx, sample_mins=self._resampling_mins)
                        aggregateData.append(df_int)
                except Exception as e:
                    utilities.log.warn(f'Potentially Non Fatal Interpolation procedure status for station {station} {e}')
                    pass
                #tm.sleep(2) # sleep 2 secs
                # On input dx==nan, the old method bombs preventing a nan form entering aggregateData
            #except RuntimeError as ex: # This is primarily here to trap DAP Failures for TDS/running on k8s
            #    message = template.format(type(ex).__name__, ex.args)
            #    utilities.log.warn(f'RuntimeError: Skipping all station for this time period/url. {station}, msg {message}')
            #    break
            except Exception as ex:
                excludedStations.append(station)
                message = template.format(type(ex).__name__, ex.args)
                traceback.print_exc()
                utilities.log.warn(f'Possibly the station simply had no data; Skip {station}, msg {message}')
        if len(aggregateData)==0:
            utilities.log.warn('No site data was found for the given site_id list. Perhaps the server is down or file doesnt exist')
            ##return np.nan
            #sys.exit(1) # Keep processing the remaining list
        utilities.log.info('{} Stations were excluded'.format(len(excludedStations)))
        utilities.log.info('{} Stations included'.format(len(aggregateData)))
        try:
            df_data = pd.concat(aggregateData, axis=1)
            idx = df_data.index
            utilities.log.info('Check for time duplicates')
            if idx.duplicated().any():
                utilities.log.info("Duplicated data times found . will keep first value(s) only")
                df_data = df_data.loc[~df_data.index.duplicated(keep='first')]
            if len(idx) != len(df_data.index):
                utilities.log.warning(f'had duplicate times {len(idx)} {len(df_data.index)}')
            #df_data.dropna(how='all', axis=1, inplace=True)
            df_data = replace_and_fill(df_data)
            df_data = df_data.sort_index()
        except Exception as e:
            utilities.log.error(f'Station list aggregation: error: {e}')
            pass
            #df_data=np.nan
        #df_data = df_data.sort_index()
        return df_data

    def aggregate_station_metadata(self)->pd.DataFrame:
        """
        Loop over the list of stations and aggregate the metadata using the method of the relevant subclasses. Then concatenate info single dataframe
        Transpose final data to have stations as index
        nans now get converted to the value in GLOBAL_FILL_VALUE

        Returns:
            Aggregated meadata: A dataframe (stations x metacolumns) for all input stations
        """
        aggregateMetaData = list()
        excludedStations = list()
        template = "A metadata exception of type {0} occurred. Arguments:\n{1!r}"
        for station in self._stations:
            try:
                dx = self.fetch_single_metadata(station)
                aggregateMetaData.append(dx)
                #tm.sleep(2) # sleep 2 secs
                #utilities.log.debug('Iterate: Kept station is {}'.format(station))
            except Exception as ex:
                excludedStations.append(station)
                message = template.format(type(ex).__name__, ex.args)
                utilities.log.warn(f'Error Value: Metadata: {station}, msg {message}')
        if len(aggregateMetaData)==0:
            utilities.log.warn('Metadata: No site data was found for the given site_id list. Perhaps the server is down Exit')
            #sys.exit(1) # Process remaining list
        utilities.log.info(f'{len(excludedStations)} stations were excluded')
        df_meta = pd.concat(aggregateMetaData, axis=1).T
        #df_meta.dropna(how='all', axis=1, inplace=True)
        df_meta = replace_and_fill(df_meta)
        return df_meta

#####################################################################################
##
## Fetching the ADCIRC NODE data from TDS or a local filesystem
##

## The fort61 file contains INTERPOLATED values of the solution to a specific lon,lat point.  
## For the fort.63, we are extracting time series from specific node numbers.  
## Unless the lon,lat point is very close to the specified node, these will be slightly different.

class adcirc_fetch_data(fetch_station_data):
    """
    Fetching WL data from ADCIRC can be done in one of two ways. The default approach is based
    on the fort.61.nc type solution where lat/lons/WL are extracted from the ADCIRC netCDF4 fort.61.nc file. This
    approach simply requires a list of stationids for query. This returns WLs that are interpolations of the ADCIRC
    grid data. 

    An alternative approach is based on using fort.63.nc data. To use the fort.63_style approach requires a more
    informative input list of stations.A dataframe with (at least) two columns with headers: stationid and nodeid where nodeid is the pre-established
    grid-specific ADCIRC nodeid corresponding to the station. Required minimum column headers: (stationid, Node)
    The nodeid indexing is that used by ADCIRC and is relative to starting at the value 1 !

    Currently available products:
    water_level (default)

    Parameters:
        station_id_list: list of station_ids to pass to ADCIRC
        periods: <list> of valid ADCIRC urls tuples (*63.nc,*.61.nc) for aggregation 
        fort63_style: <bool> If True use the fort.63-based approach
                    If True then station_id_list: a CSV file containing columns of, at least, stationid and nodeid. 
    """

    products={ 'water_level':'water_level'  # 6 min
            }

    def _remove_empty_url_pointers(self, in_periods)->list:
        """
        Loop through the entire list of adcirc urls and remove any entries that report a File Not Found error
        We expect to be able to remove many urls. The only time a hard failure occurs is if all urls are not found

        Parameters:
            in_periods: <list(str)> of proposed ADCIRC file urls
        Returns:
            new_periods: <list(str)> of proposed urls that wee actually found at the source
        """

        new_periods=list()
        for url in in_periods:
            try:
                nc = nc4.Dataset(url)
                new_periods.append(url)
            except OSError as e:
                utilities.log.warn('URL not found: Remove url {}'.format(url))
        return new_periods

## TODO this cast method is is clumsy
    def type_ADCIRC_cast(self, url, df)->str:
        """
        Attempt to determine if the input adcirc data (df) was a Forecast or a Nowcast type dataset by simply 
        examining the associated url. Ths method can assign hurricane or synoptic datasets. This method is generally
        used to build consistent metadata objects for AST  

        Attempt to find the url starttime value from the url name. Ascertain if this starttime is < or > the actual entry time in the df.
        Simply compare the last timeseries value to the url starttime. if timeseries > starttime than it is a forecast else a nowcast

        url TIME are expected to reside in url.split('/')[-6]. If that entry cannot be assigned to a datetime (ValueError), 
        it is checked if it could be an advisory (int) and thus a huricane. 


        If the input url is a LOCAL data file, then [-6] probably doesn't exist we can make no assumptions about length of name or nomenclature
        Just assume it is a FORECAST if indexing errors occur


        Assumes the times series are ordered.

        If the input url is a LOCAL data file, then we can make no assumptions about length of name or nomenclature
        Just assume it is a FORECAST if indexing errors occur

        Parameters:
            url: <str> The ADCIRC netCDG4 url to compare
            df: <DataFrame>. The ADCIRC WL read from the associated url

        Returns:
              Either NOWCAST or FORECAST
        """
        # short-circuiting this entire code
        return 'NOWCAST'
        # short-circuiting this entire code

        timeseries = pd.to_datetime(df.index.astype(str)) # Account for ctime/calender changes in pandas. Thx !.

        #  Too short and it must be local
        try:
            starttime=url.split('/')[-6]
        except IndexError as e:
            utilities.log.info(f'Found (probably) a LOCAL netCDF file: Assume a FORECAST {e}')
            return 'FORECAST'
        except Exception as e:
            utilities.log.error(f'Error: {e}')
            raise
            #sys.exit(1)

        #Synoptic ?
        try:
            urltime = dt.datetime.strptime(starttime,'%Y%m%d%H')
            if timeseries[-1] > urltime:
                return 'FORECAST'
            else:
                return 'NOWCAST'
        except ValueError:
            utilities.log.warn(f'Synoptic time check suggests this must be a Hurricane')
            pass
        except Exception as e:
            utilities.log.error(f'synoptic time check hard failed: Error: {e}')
            raise
            #sys.exit(1)

        # Hurricane ?
        try:
            urltime = starttime
            advnum = int(urltime)
            utilities.log.info(f'Found Hurricane Advisory value: Assumes forecast {advnum}')
            try:
                if url.split('/')[-2] == 'nowcast':
                    return 'NOWCAST'
                else:
                    return 'FORECAST'
            except IndexError:
                    utilities.log.info('Hurricane URL but could not determine CAST status')
                    return 'NOWCAST'
        except ValueError:
            utilities.log.warn(f'Hurricane advisory check failed: Must be local')
            return 'NOWCAST'
        except Exception as e:
            utilities.log.error(f'Hurricane advisory check hard failed: Error: {e}')
            raise
            #sys.exit(1)

#TODO change name periods to urls

##
## ADCIRC source subclass
##
    def __init__(self, station_id_list, periods=None, product='water_level',
                datum='MSL', sitename=None, gridname=None, castType=None, resample_mins=15,
                fort63_style=False, variable_name='zeta', keep_earliest_url=True):
        """
        Instantiate the ADCIRC product layer. 
        Parameters:
            station_id_list: <list(str/tuple)> input stations /station & node
            periods: <list(str)> list of input URLs that span the desired time range
            product: <str> (default 'water_level') input generic product name
            datum: Not used here
            sitename: <str> (opt) used for amending metadata only
            gridname: <str> (opt) used for metadata only
            castType: <str> (opt) used for amending metadata only
            resample_mins: <int> resample time. Set to 0 for full resolution
            fort63_style: <bool> If true input station_ids as list of tuples (stationid/nodeid)
            variable_name: <str> (default='zeta') If using a SWAN type file then you must change the variable name here
            keep_earliest_url: This boolean can be used to remove the earliest url in a postchecked list of urls 
        """
        self._product=self.products[product]
        #self._interval=interval 
        self._units='metric'
        self._datum=datum
        periods=periods
        if castType.upper()==None:
            utilities.log.info('ADCIRC: castType not set. Will result in poor metadata NAME value')
        self._castType=castType 
        if gridname==None:
            utilities.log.info('ADCIRC: gridname not specified. Will result in poor metadata NAME value') 
        self._gridname=gridname
        if sitename==None:
            utilities.log.info('ADCIRC: sitename not specified. Will result in poor metadata NAME value')
        self._sitename=sitename
        self._variable_name=variable_name
        utilities.log.info('Variable name is {}'.format(self._variable_name))

        if fort63_style:
            utilities.log.debug('Fetch station ids using fort.63 style')
            available_stations = self._fetch_adcirc_nodes_from_fort63_input_file(station_id_list)
        else:
            utilities.log.debug('Fetch station ids using fort.61 style')
            available_stations = self._fetch_adcirc_nodes_from_fort61_input_file(station_id_list, periods)
        if available_stations==None:
            utilities.log.error('No valid fort file was found: Abort')
            sys.exit(1)
        self.available_stations_tuple=available_stations
        utilities.log.info(f'List of ADCIRC stations {self.available_stations_tuple}')
        periods = self._remove_empty_url_pointers(periods)
        if not keep_earliest_url:
            remperiod,periods=periods[0],periods[1:]
            print(f' FINAL URLS {periods}')
            utilities.log.info(f'Removing earliest confirmed URL from the processing list: remaining is {len(periods)}: Removed url was {remperiod}') 
        super().__init__(self.available_stations_tuple, periods, resample_mins=resample_mins) # Pass in the full dict

    def _fetch_adcirc_nodes_from_fort63_input_file(self, station_df) -> list():
        """

        The Node index is decremented by one to better share subsequent code

        Parameters:
            station_csv <str>. A list of station ids/Nodes in DataFrame format
            periods <list>. The list of url63 values. 

        Returns: list of tuples (stationid,nodeid). Superfluous stationids are ignored
        """
        utilities.log.debug('Attempt to find ADCIRC fort_63 stations/Nodes')
        try:
            idx=list()
            utilities.log.debug('Fetch stations fort63 style ...')
            station_df['NodeMinusOne']=station_df["Node"]-1 # decrease nodeid by one to mimic the fort.61 indexing in subsequent code
            station_ids = station_df["stationid"].tolist()
            node_idx = station_df["NodeMinusOne"].tolist()
            idx = (list(zip(station_ids, node_idx)))
            return idx
        except Exception as e:
            utilities.log.error(f'fort_63_style. Input file problematic {station_df} {e}: Abort')
            raise
            #sys.exit(1)

    def _fetch_adcirc_nodes_from_fort61_input_file(self, stations, periods) -> list(): 
        """
        periods contains all the possible urls. We do this because TDS may or may not actually
        have one or more of the requested urls. So we keep checking urls for stations until no more
        urls exist. If none, then die.

        Parameters:
            station <str>. A list of (eg NOAA/Contrails) station ids
            periods <list>. The list of url-61 values. 

        Returns: list of tuples (stationid,nodeid). Superfluous stationids are ignored
        """
        utilities.log.info('Attempt to find ADCIRC stations')
        full_idx=list()
        for url61 in periods:
            utilities.log.info('Fetch stations: {} '.format(url61))
            try:
                ds = xr.open_dataset(url61)
                ds = ds.transpose()
                ds.attrs = []
                sn = ds['station_name'].values
                snn = []
                for i in range(len(sn)): # This gets the stationids IN THE FILE not necc what we requested.
                    ts = str(sn[i].strip().decode("utf-8"))
                    snn.append(str(ts.split(' ')[0])) # Strips off actual name leaving only the leading id
                # Get intersection of input station ids and available ids
                snn_pruned=[str(x) for x in stations if x in snn]
                idx = list() # Build a list of tuples (stationid,nodeid)
                for ss in stations: # Loop over stations to maintain order
                    s = ss # Supposed to pull out the first word
                    # Cnvert to a try clause
                    if s in snn_pruned:
                        utilities.log.debug("{} is in list.".format(s))
                        idx.append( (s,snn.index(s)) )
                    else:
                        utilities.log.info("{} not in fort.61.nc station_name list".format(s))
                        ##sys.exit(1)
                full_idx+=idx
            except OSError:
                utilities.log.warn("Could not open/read a specific fort.61 URL. Try next iteration {}".format(url61))
            except Exception as e:
                utilities.log.error('Could not find ANY fort.61 urls from which to get stations lists {}'.format(e))
                utilities.log.info('Bottomed out in _fetch_adcirc_nodes_from_fort61_input_file')
                raise
                #sys.exit(1) 
        # Remove any duplicate tuples eg if len(periods) > 1
        full_idx = list(set([i for i in full_idx]))
        return full_idx

##
## Added in a retry clause here. In June/2023, we've observed that occasionally, a nowcast will fail to read even when 
## subsequent manual checking indicates aexistance of a valid file. We've tentatively assigned this either to a FS timing issue
## or a file locking issue. So, if we get a read failure here, we wait and try again one-time. Then fail on a failed 2nd try
##
    def fetch_single_product(self, station_tuple, periods) -> pd.DataFrame:
        """
        variables.key() are now checked to try and determine if the dataset has been transposed (and rechunked) or not.
        If so, then fetching of the timeseries will be much, much faster but also requires chaning the code here a little

        Parameters:
            station_tuple (<str>,<int>). A tuple that maps stationid to the current ADCIRC-grid nodeid
            periods <list>. A url-61 values. 

        Returns: dataframe of time (timestamps) vs values for the requested stationid
        """
        # First up. Get the list of available station ids

        station=station_tuple[0]
        node=station_tuple[1]
        datalist=list()
        typeCast_status=list() # Check each period to see if this was a nowcast or forecast type fetch. If mixed then abort

        WAITTIME=os.getenv('AST_IO_RETRY_PAUSE') if os.getenv('AST_IO_RETRY_PAUSE') is not None else '30'
        try:
            WAITTIME=int(WAITTIME)
        except ValueError as e:
            utilities.log.info(f' Failed to convert AST_IO_RETRY_PAUSE to seconds: Was {WAITTIME}. Will use default {e}')
            WAITTIME=30

        for url in periods: # If a period is SHORT no data may be found which is acceptible 
            waitsec=0 # Start off assuming no wait required
            try:
                nc = nc4.Dataset(url)
            except Exception as e: # Catch all errors as we are not sure of the complete set of possibles
                utilities.log.exit(f'Failed try to open a netCDF object: This should never happen so abort. Stationid {station}. error {e}') 
                sys.exit(1)

            # We have not seen dap failures occur when querying for global data info so don't worry about it here 
            data_transposed = nc[self._variable_name].dimensions[0] == 'node'
            if data_transposed:
                utilities.log.info('Expecting netCDF data in transposed form')
            if self._variable_name not in nc.variables.keys():
                print(f'{self._variable_name} not found in netCDF for {url}')
                # okay to have a missing one  do not exit 
                # sys.exit(1)
            else:
                for itry in range(1): # Try a maximum of 2 times
                    got_data=False
                    time_var = nc.variables['time']
                    t = nc4.num2date(time_var[:], time_var.units)
                    data = np.empty([len(t), 0])
                    try:
                        if nc[self._variable_name].dimensions[0] == 'time':
                            data = nc[self._variable_name][:,node] 
                            got_data=True
                            break
                        elif nc[self._variable_name].dimensions[0] == 'node':
                            data = nc[self._variable_name][node] 
                            got_data=True
                            break
                        else:
                            utilities.log.error(f'Unexpected leading variable name {ds.variables[v].dims}')
                            #sys.exit(1)
                    except Exception as e:
                         waitsec+=WAITTIME
                         utilities.log.warn(f'{itry} attempt to fetch URL data failed. Wait {waitsec}s and try again. {e}')
                if got_data:
                    np.place(data, data < -1000, np.nan)
                    dx = pd.DataFrame(data, columns=[str(node)], index=t)
                    dx.columns=[station]
                    dx.index.name='TIME'
                    typeCast_status.append(self.type_ADCIRC_cast(url, dx))
                    dx.index = pd.to_datetime(dx.index.astype(str)) # New pandas can only do this to strings now
                    datalist.append(dx)
                else:
                    raise # Let the aggregate caller choose to ignore this missing station and all associated urls
                    #sys.exit()
        try:
            df_data = pd.concat(datalist)
        except Exception as e:
            print(f'ADCIRC concat error: {e}')
            raise # These get trapped/ignored later
        # Check if ALL entries in typeCast_status are the same. If not fail hard.
        if len(set(typeCast_status)) != 1:
            utilities.log.error('Some mix up with typeCast_status {}'.format(typeCast_status))
            #sys.exit(1)
        self._typeCast = list(set(typeCast_status))[0] 
        #utilities.log.info('ADCIRC typeCast determined to be {}'.format(self._typeCast))
        return df_data
##
## The nodelat/nodelon objects are masked arrays. For a single node (as used here)
## the ma.getdata() returns an ndarray of shape=() but with the a single value.
## Imposing a float() onto that value converts it to a real float
## Try to determine if this was a nowcast or a forecast type dataset

    def fetch_single_metadata(self, station_tuple) -> pd.DataFrame:
        """
        variables.key() are now checked to try and determine if the dataset has been transposed (and rechunked) or not.
        If so, then fetching of the timeseries will be much, much faster but also requires chaning the code here a little

        Parameters:
            station <str>. A valid station id

        Returns:
            dataframe of time (timestamps) vs values for the requested nodes 
        """
        station=station_tuple[0]
        node=station_tuple[1]
        meta=dict()
        periods=self._periods

        for url in periods:
            nc = nc4.Dataset(url)
            # we need to test access to the netCDF variables, due to infrequent issues with
            # netCDF files written with v1.8 of HDF5.
            try:
                nodelon=nc.variables['x'][node]
                nodelat=nc.variables['y'][node]
                break; 
            except IndexError as e:
                utilities.log.error(f' Potentially non-fatal: Meta not found. Trying another URL was {url}:{e}')
                #sys.exit()
        try:
            lat = float(ma.getdata(nodelat))
            lon = float(ma.getdata(nodelon))
            meta['LAT'] = lat 
            meta['LON'] = lon 
            # meta['NAME']= nc.agrid # Long form of grid name description # Or possible use nc.version
            #meta['NAME']='_'.join([self._gridname.upper(),self._typeCast.upper()]) # These values come from the calling routine and should be usually nowcast, forecast
            meta['NAME']=str(station)
            #meta['VERSION'] = nc.version
            meta['UNITS'] ='m'
            meta['TZ'] = GLOBAL_TIMEZONE # Can look in nc.comments
            meta['OWNER'] = nc.source
            meta['STATE'] = np.nan 
            meta['COUNTY'] = np.nan 
            meta['SITE'] = self._sitename
            meta['CAST'] = self._typeCast.upper()
            df_meta=pd.DataFrame.from_dict(meta, orient='index')
            df_meta.columns = [str(station)]
        except IndexError as e:
            utilities.log.error(f'Failed updating te ADCIRC station metadata for station {station}, {e}')
            #sys.exit(1)
        return df_meta

#####################################################################################
##
## NOAA/NOS source subclass
##

class noaanos_fetch_data(fetch_station_data):
    """
    Parameters:
        station_id_list: list of NOAA station_ids <str>
        a tuple of (time_start, time_end) <str>,<str> format %Y-%m-%d %H:%M:%S
        a valid PRODUCT id <str>: hourly_height, water_level,predictions (tidal predictions)
        interval <str> set to 'h' return hourly data, else 6min data

        Note: hourly_height data only appear for station after some time period (not sure how long that is). 

        NOTE: Default to using imperial units. Because the metadata that gets returned only reports
        the units for how the data were stored not fetched. So it wouid be easy for the calling program to get confused.
        Let the caller choose to update units and modify the df_meta structure prior to DB uploads

        Two dicts are used to manage jobs. The first (products) maps generic product names used by high level codes
        to the specific product names in NOAA/NOS, The second (noaa_data_column_names) is used here internally to properly select 
        the column name of the data

        UNITS listed as: https://api.tidesandcurrents.noaa.gov/api/prod/#units. Note this implies a hybrid MKS/CGS system and not MKS.

        Currently tested input products:
        water_level (default)
        predictions (Tidal predictions) 
        air_pressure
        hourly_height
        wind_speed
        
    """
    # dict( persistant tag: source specific tag )
    # products defines current products (as keys) and uses the value as a column header in the returned data set

    # NOTE: This dict maps the generic input data type (key) to the actual product name used by noaa-coops
    products={ 'water_level':'water_level',  # 6 min
               'predictions': 'predictions', # 6 min
               'air_pressure': 'air_pressure',
               'hourly_height':'hourly_height', # hourly
               'wind_speed':'wind'}

    # NOTE: This dict maps the input data type (key) to the COLUMN NAME that gets returned by noaa-coops
    # this value is not always the product name
    noaa_data_column_names={ 'water_level':'water_level',  # 6 min
               'predictions': 'predicted_wl', # 6 min
               'air_pressure': 'air_press',
               'hourly_height':'water_level', # hourly
               'wind':'spd'}

    def __init__(self, station_id_list, periods, product='water_level', interval=None, units='metric', 
                datum='MSL', resample_mins=15):
        """
        Invoke the NOAA subclass

        Parameters:
            station_id_list: <list> List of desired station ids
            periods: list of time-tuples indicating the desired time range(s)
            product: <str> (Default='water_level')  The generic product name
            interval: <str> NOAA coops specific param. None or 'h'
            datum: <str> (Default='MSL')
            resample_mins: <int> time sampling. Specify 0 to get maximum resolution
        """
        self._data_unit=map_product_to_harvester_units(product)
        try:
            self._product=self.products[product] # self.products[product] # product
            utilities.log.info(f'NOAA Fetching product {self._product}')
        except KeyError as e:
            utilities.log.error(f'NOAA/NOS No such product key. Input {product}, Available {self.products.keys()}: {e}')
            raise
            #sys.exit(1)
        else:
            self._interval=interval
        self._units='metric' # Redundant cleanup TODO
        self._datum=datum
        super().__init__(station_id_list, periods, resample_mins=resample_mins)

    def check_duplicate_time_entries(self, station, stationdata):
        """
        Sometimes station data comes back with multiple entries for a single time.
        Here we search for such dups and keep the FIRST one (Same as for ADDA)
        Choosing first was based on a single station and looking at the noaa coops website

        Parameters:
            station: <str> an individual stationID to check
            stationData: <dataframe>. Current list of all station product levels (from detailedIDlist) 
        Returns:
            New dataframe containing no duplicate values
            multivalue. bool: True if duplicates were found 
        """
        multivalue = False
        idx = stationdata.index
        if idx.duplicated().any():
            utilities.log.info(f'Duplicated Obs data found for station {str(station)} will keep first value(s) only')
            stationdata = stationdata.loc[~stationdata.index.duplicated(keep='first')]
            multivalue = True
        return stationdata, multivalue

# The weirdness with tstart/tend. Prior work by us indicated noaa coops requires time formats of (%Y%m%d %H:%M')
# Even though their website says otherwise (as of Oct 2021)

    def fetch_single_product(self, station, time_range) -> pd.DataFrame:
        """
        For a single NOAA NOS site_id, process all the tuples from the input periods list
        and aggregate them into a dataframe with index pd.timestamps and a single column
        containing the desired product values. Rename the column to station id
        
        NOAA COOPS does not have the same time range constraints as Contrails. So this tuple list
        can, in fact, simply be the start and end time.
        
        Parameters:
            station <str>. A valid station id
            time_range <tuple>. Start and end times (<str>,<str>) denoting time ranges

        Returns:
            dataframe of time (timestamps) vs values for the requested station
        """
        tstart,tend=time_range
        utilities.log.info('NOAA/NOS:Iterate: start time is {}, end time is {}, station is {}'.format(tstart,tend,station))
        timein =  pd.Timestamp(tstart).strftime('%Y%m%d %H:%M')
        timeout =  pd.Timestamp(tend).strftime('%Y%m%d %H:%M')
        try:
            stationdata = pd.DataFrame()
            location = coops.Station(station)
            dx = location.get_data(begin_date=timein,
                                            end_date=timeout,
                                            product=self._product,
                                            datum=self._datum,
                                            units=self._units,
                                            interval=self._interval, # If none defaults to 6min
                                            time_zone=GLOBAL_TIMEZONE)[self.noaa_data_column_names[self._product]].to_frame()
            df_data, multivalue = self.check_duplicate_time_entries(station, dx)
            # Put checks in here in case we want to exclude stations with determined multiple values
            df_data.reset_index(inplace=True)
            df_data.set_index(['date_time'], inplace=True)
            df_data.columns=[station]
            df_data.index.name='TIME'
            df_data.index = pd.to_datetime(df_data.index)
        except ConnectionError as ec:
            utilities.log.error(f'Hard fail: Could not connect to COOPS for products {station}: {ec}')
        except HTTPError as eh:
            utilities.log.error(f'Hard fail: HTTP error to COOPS for products: {eh}')
        except Timeout:
            utilities.log.error('Hard fail: Timeout')
        except Exception as e:
            utilities.log.error(f'Hard Error: NOAA/NOS data error: station {station}: {e} was {self._product}')
        try:
            df_data=df_data.astype(float)
        except Exception as e:
            utilities.log.warning(f'NOAA/NOS station data warning: {e}')
            df_data = np.nan
        return df_data

# TODO The NOAA metadata scheme is Horrible for what we need. This example is very tentative 
    def fetch_single_metadata(self, station) -> pd.DataFrame:
        """
        For a single NOAA site_id fetch the associated metadata.
        The choice of data is highly subjective at this time.

        Parameters:
             A valid station id <str>
        Returns:
             dataframe of preselected metadata for a single station in the (keys,values) orientation

             This orientation facilitates aggregation upstream. Upstream will transpose this eventually
             to our preferred orientation with stations as index
        """
        meta=dict()
        try:
            location = coops.Station(station)
            meta['LAT'] = location.metadata['lat'] if location.metadata['lat']!='' else np.nan
            meta['LON'] = location.metadata['lng'] if location.metadata['lng']!='' else np.nan
            meta['NAME'] =  location.metadata['name'] if location.metadata['name']!='' else np.nan
            meta['UNITS'] = self._data_unit # UNITS # Manual override bcs -> location.sensors['units'] # This can DIFFER from the actual data. For data you can specify a transform to metric.
            #meta['ELEVATION'] = location['elevation']
            meta['TZ'] = GLOBAL_TIMEZONE
            meta['OWNER'] = 'NOAA/NOS'
            meta['STATE'] = location.metadata['state'] if location.metadata['state']!='' else np.nan
            meta['COUNTY'] = np.nan # None
            #
            df_meta=pd.DataFrame.from_dict(meta, orient='index')
            df_meta.columns = [str(station)]
        except Exception as e:
            utilities.log.exception(f'NOAA/NOS meta error station {station}: {e}')
            #sys.exit(1)
        return df_meta

#####################################################################################
##
## CONTRAILS source subclass
## Fetching the Station data from Contrails managed by OneRain
##

## Must MANUALLY convert to metric here

class contrails_fetch_data(fetch_station_data):
    """
    Parameters:
        station_id_list: list of station_ids <str>
        a tuple of (time_start, time_end) <str>,<str> format %Y-%m-%d %H:%M:%S
        a valid OWNER for Contrails <str>: One of NCDOT,Lake Lure,Asheville,Carolina Beach,
            Town of Cary,NCEM Synthetic,Wake County,
            USFWS,Morrisville,NCEM,USGS,NOAA,Currituck County,Duke Energy
        config: a dict containing values for domain <str>, method <str>, systemkey <str>
        a valid PRODUCT id <str>: See CLASSDICT definitions for specifics

        NOTE: Defaults to using imperial units. Manually correct them 

        Two dicts are used to manage jobs. The first (products) maps generic product names used by high level codes
        to the specific product names in NOAA/NOS, The second (CLASSDICT) is used here internally map names to
        the data index used by Contrails

        Moreover, river guages and coastal guages get treated differently

        Currently tested input products:
        river_water_level 
        river_flow_volume
        river_stream_elevation
        coastal_water_level
        air_pressure
    """
    # dict( persistant tag: source speciific tag )

# See Tom's email Regarding coastal versus river class values
    products={ 'river_water_level':'Stage', 
               'river_flow_volume':'Flow Volume',
               'coastal_water_level':'Water Elevation',
               'air_pressure':'Barometric Pressure', 
               'river_stream_elevation':'Stream Elevation'
             }

    CLASSDICT = {
        'Rain Increment':10,
        'Rain Accumulation':11,
        'Stage':20,            # River gauges
        'Water Elevation':94,  # Coastal gauges  - not documented in contrails doc as of early 2022
        'Stream Elevation':210, # New product enabled for RENCI on Sept 2023
        'Flow Volume':25,
        'Air Temperature':30,
        'Fuel Temperature':38,
        'Wind Velocity':40,
        'Wind Velocity, maximum':41,
        'Wind Direction':44,
        'ALERT Wind':47,
        'Relative Humidity':50,
        'Soil Moisture':51,
        'Fuel Moisture':52,
        'Barometric Pressure':53,
        'Net Solar Radiation':60,
        'Evapotranspiration Rate':84,
        'Binary Status':197,
        'Repeater Status':198,
        'Battery':199,
        'Average Voltage':200,
        'Repeater Pass List':240,
        'Msg Count':246,
    }
    # Here call a pipeline to do the fetch and then super the main class
    # We expect the calling metyhod to have resolved the different MAP terms for a given source
    # Currently only tested with the NCEM owner

    def __init__(self, station_id_list, periods, config, product='river_water_level', owner='NCEM', resample_mins=15):
        """
        Invoke the Contrails subclass

        Parameters:
            station_id_list: <list> List of desired station ids
            periods: list of time-tuples indicating the desired time range(s)
            product: <str> (Default='river_water_level')  The generic product name
            interval: <str> NOAA coops specific param. None or 'h'
            owner: <str> (Default 'NCEM') a valid owner
            resample_mins: <int> time sampling. Specify 0 to get maximum resolution
        """

        self._owner=owner
        self._data_unit=map_product_to_harvester_units(product)
        try:
            self._product=self.products[product] # product
        except KeyError:
            utilities.log.error('Contrails No such product key. Input {}, Available {}'.format(product, self.products.keys()))
            raise
            ##sys.exit(1)
        print(self._product)
        utilities.log.info(f'CONTRAILS Fetching product {self._product}')
        self._systemkey=config['systemkey']
        self._domain=config['domain']
        super().__init__(station_id_list, periods, resample_mins=resample_mins)

    # Customized splitting of the timerange into a list of day-centric tuples.

    def return_list_of_daily_timeranges(self, time_tuple)-> list():
        """
        Take an arbitrary start and endtime (inclusive) in the format of %Y-%m-%d %H:%M:%S. Break up into a list of tuples which 
        are at most a day in length AND break along day boundaries. [ {day1,day1),(day2,day2)..]
        The first tuple and the last tuple can be partial days. All intervening tuples will be full days.
    
        Assume an HOURLY stepping even though non-zero minute offsets may be in effect.

        Parameters:
            A tuple consisting of:
            start_time: <str> Time of format %Y-%m-%d %H:%M:%S
            end_time: <str> Time of format %Y-%m-%d %H:%M:%S
    
        Returns:
            periods: List of daily tuple ranges
       """
        start_time=time_tuple[0]
        end_time=time_tuple[1]
        periods=list()
        dformat='%Y-%m-%d %H:%M:%S'
        #print(f'Parameters: start time {start_time}, end_time {end_time}')
    
        time_start = dt.datetime.strptime(start_time, dformat)
        time_end = dt.datetime.strptime(end_time, dformat)
        if time_start > time_end:
            #print('Swapping input times')
            time_start, time_end = time_end, time_start
    
        today = dt.datetime.today()
        if time_end > today:
              time_end = today
              print(f'Contrails: Truncating list: new end time is {dt.datetime.strftime(today, dformat)}')
    
        #What hours/min/secs are we starting on - compute proper interval shifting
        init_hour = 24-math.floor(time_start.hour)-1
        init_min = 60-math.floor(time_start.minute)-1
        init_sec = 60-math.floor(time_start.second)-1
    
        oneSecond=dt.timedelta(seconds=1) # An update interval shift
    
        subrange_start = time_start
        while subrange_start < time_end: # No <= here to ensure we do not try to span multiple days
            interval = dt.timedelta(hours=init_hour, minutes=init_min, seconds=init_sec)
            subrange_end=min(subrange_start+interval,time_end) # Need a variable interval to prevent a day-span  
            periods.append( (dt.datetime.strftime(subrange_start,dformat),dt.datetime.strftime(subrange_end,dformat)) )
            subrange_start=subrange_end+oneSecond # onehourint
            init_hour, init_min, init_sec = 23,59,59
        return periods

    def build_url_for_contrails_station(self, domain, systemkey, indict)->str:
        """
        Build a simple query for a single gauge and the product level values
        Parameters:
            domain: <str> contrails domain
            systemkey: <str> contrails authorization key
            indict: <dict> of parameters for the the final url
        Returns:
            full_url: <str> A fully formatted Contrails URL
        """
        url=domain
        url_values=urllib.parse.urlencode(indict)
        full_url = url +'?' +url_values
        return full_url

    def convert_to_metric(self, df)-> pd.DataFrame:
        """
        Contrails returns moost (all?) data in imperial units
        Here we convert valid productsa to metric
        The product selection is based on the native Contrails product names
        which is carried by self.product

        arbitrary_min = 600. Contrails can return pressures in mb OR inHg with little warning.
            depending on the station. We can't check here so try to deduce which units the data are in
      
        Dataframe is updated inplace

        Parameters
            df: <dataframe> Data of tmes x product for a specific station
        Returns:
            df: <dataframe> Data of times x product in metric units
        """ 
        arbitrary_min = 600 
        product = self._product
        df=df.astype(float)
        if product == 'Stage' or product == 'Water Elevation' or product == 'Stream Elevation':
            utilities.log.info('Contrails. Converting to meters')
            df=df * 0.3048 # feet to meters
            return df
        if product == 'Flow Volume':
            df=df * (0.3048**3)
            return df
        if product == 'Barometric Pressure':
            test_val = max(df.values)
            if test_val < arbitrary_min:
                utilities.log.info('Contrails. Converting inHg to millibars (atm)')
                df=df * 1000.0 / 29.52998 
            return df
        utilities.log.error('convert_to_metric: Dropped out the bottom. Unexpected product of {}: Abort'.format(product))
        sys.exit(1)

## Now implement the required two methods

    def fetch_single_product(self, station, time_range) -> pd.DataFrame: 
        """
        For a single Contrails site_id, process all tuples from the input periods list
        and aggregate them into a dataframe with index pd.timestamps and a single column
        containing the desired CLASSDICT[...] values. Rename the column to station id
        
        Parameters:
            station <str>. A valid station id
            time_range <tuple>. Start and end times (<str>,<str>) denoting time ranges

        Returns:
            dataframe of time (timestamps) vs values for the requested station
        """
        METHOD = 'GetSensorData'
        datalist=list()
        periods = self.return_list_of_daily_timeranges(time_range)
        for tstart,tend in periods:
            utilities.log.debug('Iterate: start time is {}, end time is {}, station is {}'.format(tstart,tend,station))
            indict = {'method': METHOD, 'class': self.CLASSDICT[self._product],
                 'system_key': self._systemkey ,'site_id': station,
                 'tz': GLOBAL_TIMEZONE,
                 'data_start': tstart,'data_end': tend }
            url = self.build_url_for_contrails_station(self._domain,self._systemkey,indict)
            try:
                response = requests.get(url)
                dict_data = xmltodict.parse(response.content)
                data = dict_data['onerain']['response']['general']
                dx = pd.DataFrame(data['row']) # must be <= 5000 entries returned
                dx = dx[['data_time','data_value']]
                utilities.log.info('Contrails. Converting to meters')
                dx.columns = ['TIME',station]
                dx.set_index('TIME',inplace=True)
                dx.index = pd.to_datetime(dx.index)
                datalist.append(dx)
            except Exception as e:
                utilities.log.warn(f'Contrails response data error: Perhaps empty data contribution: {e}')
        try:
            # Manually convert all values to meters
            df_data = pd.concat(datalist)
            utilities.log.info('Contrails. Converting to meters')
            df_data = self.convert_to_metric(df_data)
        except Exception as e:
            utilities.log.warn('Contrails failed concat: error: {}'.format(e))
            df_data=np.nan
        return df_data

# According to oneRain the current best way to access the meta data is using or_site_id
# Also river and coastal metadata return different kind of objects

    def fetch_single_metadata(self, station) -> pd.DataFrame:      
        """
        For a single Contrails site_id fetch the associated metadata.
        Need to perform multiple queries to get the desired set of data. This is optional
        The caller will check the DB to see if this a new station requiring metadata

        Parameters:
             A valid station id <str>
        Returns:
             dataframe of preselected metadata for a single station in the (keys,values) orientation

             This orientation facilitates aggregation upstream. Upstream will transpose this eventually
             to our preferred orientation with stations as index
        """
        meta=dict() 
        # 1
        METHOD = 'GetSensorMetaData'
        indict = {'method': METHOD,'tz':GLOBAL_TIMEZONE, 'class': self.CLASSDICT[self._product],
             'system_key': self._systemkey ,'site_id': station }
        url = self.build_url_for_contrails_station(self._domain,self._systemkey,indict)
        response = requests.get(url)
        dict_data = xmltodict.parse(response.content)
        data = dict_data['onerain']['response']['general']['row']
        if isinstance(data, list):
            for entry in data:
                if entry['sensor_class']==self.CLASSDICT[self._product]:
                    list_sensor_id = entry['or_sensor_id'] # Yes a list of ordered dicts.
                    or_site_id = entry['or_site_id']
                    break
        else:
            or_site_id= data['or_site_id']
        utilities.log.info(f'Current or_site_id is {or_site_id}')
        # 2
        METHOD = 'GetSiteMetaData'
        #indict = {'method': METHOD,'tz':GLOBAL_TIMEZONE, 'class': self.CLASSDICT[self._product],
        #     'system_key': self._systemkey ,'site_id': station }
        indict = {'method': METHOD,'tz':GLOBAL_TIMEZONE, 'class': self.CLASSDICT[self._product],
             'system_key': self._systemkey ,'or_site_id': or_site_id }
        url = self.build_url_for_contrails_station(self._domain,self._systemkey,indict)
        try:
            response = requests.get(url)
            dict_data = xmltodict.parse(response.content)
            data2 = dict_data['onerain']['response']['general']['row']
            # Gets here but then fails hard and returns for GTNN7
            meta['LAT'] = data2['latitude_dec'] if data2['latitude_dec'] !='' else np.nan
            meta['LON'] = data2['longitude_dec'] if data2['longitude_dec'] !='' else np.nan
            meta['NAME'] = data['location'] if data2['location'] !='' else np.nan
            meta['UNITS'] = self._data_unit # UNITS # Manual override bcs -> data['units'].replace('.','') # I have seen . in some labels
            meta['TZ'] = GLOBAL_TIMEZONE # data['utc_offset']
            ###meta['ELEVATION'] = data['elevation']
            meta['OWNER'] = self._owner # data2 always returns the value=DEPRECATED data2['owner']
            meta['STATE'] = np.nan # None # data2['state']  # DO these work ?
            meta['COUNTY'] = np.nan
            #
            df_meta=pd.DataFrame.from_dict(meta, orient='index')
            df_meta.columns = [str(station)]
        except Exception as e:
            utilities.log.exception(f'Contrails response meta error: {e}')
            raise
            ##sys.exit(1)
        return df_meta

#####################################################################################
##
## NEW NOAA Class. This is an alternative approach  to the noaa-coops approach.
## The prior approach had suspicious missing data-gaps which no one could explain
##

class noaa_web_fetch_data(fetch_station_data):
    """
    Parameters:
        station_id_list: list of NOAA station_ids <str>
        a tuple of (time_start, time_end) <str>,<str> format %Y-%m-%d %H:%M:%S
        a valid PRODUCT id <str>: hourly_height, water_level,predictions (tidal predictions)
        interval <str> set to 'h' return hourly data, else 6min data

        Note: hourly_height data only appear for station after some time period (not sure how long that is). 

        NOTE: Default to using imperial units. Because the metadata that gets returned only reports
        the units for how the data were stored not fetched. So it wouid be easy for the calling program to get confused.
        Let the caller choose to update units and modify the df_meta structure prior to DB uploads

        Two dicts are used to manage jobs. The first (products) maps generic product names used by high level codes
        to the specific product names in NOAA/NOS, The second (noaa_data_column_names) is used here internally to properly select 
        the column name of the data

        UNITS listed as: https://api.tidesandcurrents.noaa.gov/api/prod/#units. Note this implies a hybrid MKS/CGS system and not MKS.

        Currently tested input products:
        water_level (default)
        predictions (Tidal predictions) 
        air_pressure
        hourly_height
        wind_speed
    """

# MAP AST data product names to NOAA product names
    products={ 'water_level':'water_level',  # 6 min
               'predictions': 'predictions', # 6 min
               'air_pressure': 'air_pressure',
               'hourly_height':'hourly_height', # hourly
               'wind_speed':'wind'
    }

# MAP NOAA returned header names to AST data product names
# NOTE: The Wind product returns 3 values, Speed, Direction, Gust. We currently only fetch Speed

    web_return_columns={'water_level':' Water Level', # Yes there are spaces in thar'
                        'hourly_height':' Water Level',
                        'predictions':' Prediction',
                        'air_pressure':' Pressure',
                        'wind':' Speed'
    }

    def __init__(self, station_id_list, periods, product='water_level', interval=None, units='metric',
                datum='MSL', resample_mins=15):
        """
        Invoke the noaa-web subclass

        Parameters:
            station_id_list: <list> List of desired station ids
            periods: list of time-tuples indicating the desired time range(s)
            product: <str> (Default='river_water_level')  The generic product name
            interval: <str> NOAA coops specific param. None or 'h'
            owner: <str> (Default 'NCEM') a valid owner
            resample_mins: <int> time sampling. Specify 0 to get maximum resolution
        """
        self._data_unit=map_product_to_harvester_units(product)
        try:
            self._product=self.products[product] # self.products[product] # product
            utilities.log.info(f'NOAA-WEB Fetching product {self._product}')
        except KeyError as e:
            utilities.log.error(f'NOAA/NOS-WEB No such product key. Input {product}, Available {self.products.keys()}: {e}')
            raise
            ##sys.exit(1)
        else:
            self._interval=interval
        self._units='metric' # Redundant cleanup TODO
        self._datum=datum
        self._domain='https://api.tidesandcurrents.noaa.gov/api/prod/datagetter'
        super().__init__(station_id_list, periods, resample_mins=resample_mins)

    # Customized splitting of the timerange into a list of day-centric tuples.

    def return_list_of_daily_timeranges(self, time_tuple)-> list():
        """
        Take an arbitrary start and endtime (inclusive) in the format of %Y-%m-%d %H:%M:%S. Break up into a list of tuples which 
        are at most a day in length AND break along day boundaries. [ {day1,day1),(day2,day2)..]
        The first tuple and the last tuple can be partial days. All intervening tuples will be full days.
    
        Assume an HOURLY stepping even though non-zero minute offsets may be in effect.

        Parameters:
            A tuple consisting of:
            start_time: <str> Time of format %Y-%m-%d %H:%M:%S
            end_time: <str> Time of format %Y-%m-%d %H:%M:%S
    
        Returns:
            periods: List of daily tuple ranges
       """
        start_time=time_tuple[0]
        end_time=time_tuple[1]
        periods=list()
        dformat='%Y-%m-%d %H:%M:%S'
        doformat='%Y%m%d %H:%M'
        #print(f'Parameters: start time {start_time}, end_time {end_time}')
    
        time_start = dt.datetime.strptime(start_time, dformat)
        time_end = dt.datetime.strptime(end_time, dformat)
        if time_start > time_end:
            #print('Swapping input times')
            time_start, time_end = time_end, time_start
    
        #What hours/min/secs are we starting on - compute proper interval shifting
        init_hour = 24-math.floor(time_start.hour)-1
        init_min = 60-math.floor(time_start.minute)-1
        init_sec = 60-math.floor(time_start.second)-1
    
        oneSecond=dt.timedelta(seconds=1) # An update interval shift
    
        subrange_start = time_start
        while subrange_start <= time_end:
            interval = dt.timedelta(hours=init_hour, minutes=init_min, seconds=init_sec)
            subrange_end=min(subrange_start+interval,time_end) # Need a variable interval to prevent a day-span  
            periods.append( (dt.datetime.strftime(subrange_start,doformat),dt.datetime.strftime(subrange_end,doformat)) )
            subrange_start=subrange_end+oneSecond # onehourint
            init_hour, init_min, init_sec = 23,59,59
        return periods

## TODO the url builders are redundant: Needs a refactor

    def build_url_for_noaaweb_station(self, domain, indict)->str:
        """
        Build a simple query for a single gauge and the product level values
        Parameters:
            domain: <str> contrails domain
            systemkey: <str> contrails authorization key
            indict: <dict> of parameters for the the final url
        Returns:
            full_url: <str> A fully formatted Contrails URL
        """
        url=domain
        url_values=urllib.parse.urlencode(indict)
        full_url = url +'?' +url_values
        return full_url

##
## When requesting a json or xml format back, it returns as a string (if using.text) or bytes (when using.content)
## Not sure what that is about
##

## NOTE: The response data object actually returns:
## Date Time   Water Level   Sigma   O or I (for verified)   F   R   L  Quality. We only keep the Datetime and Product
##

## Had to comment out the sys.exit(). Sometimes (eg station=1619010) no metadat gets returned as that 
## station is declared to not exist

    def fetch_single_product(self, station, time_range) -> pd.DataFrame: 
        """
        For a single NOAA site_id, process all tuples from the input periods list
        and aggregate them into a dataframe with index pd.timestamps and a single column
        
        Parameters:
            station <str>. A valid station id
            time_range <tuple>. Start and end times (<str>,<str>) denoting time ranges

        Returns:
            dataframe of time (timestamps) vs values for the requested station
        """
        datalist=list()
        periods = self.return_list_of_daily_timeranges(time_range)
        for tstart,tend in periods:
            utilities.log.debug('Iterate: start time is {}, end time is {}, station is {}'.format(tstart,tend,station))
            indict = {'product': self._product,
                 'station': station,
                 'datum':'MSL',
                 'time_zone': GLOBAL_TIMEZONE,
                 'units':'metric', 'format':'csv',
                 'application':'DataAPI_Sample',
                 'begin_date': tstart,'end_date': tend }
            url = self.build_url_for_noaaweb_station(self._domain,indict)
            try:
                response = requests.get(url)
                csvStringIO = StringIO(response.text)
                df = pd.read_csv(csvStringIO, sep=",", header=0) 
                dx=df[['Date Time',self.web_return_columns[self._product]]]
                dx.columns=['TIME',self._product]
                dx=dx.set_index('TIME')
                dx.index = pd.to_datetime(dx.index)
                dx.columns=[station]
                dx.columns=[station]
                datalist.append(dx)
            except Exception as e:
                utilities.log.warn(f'noaa-web response data error: Perhaps empty data contribution: station {station}: {e} ')
        try:
            df_data = pd.concat(datalist)
        except Exception as e:
            utilities.log.warn('noaa-web failed concat: error: {}'.format(e))
            df_data=np.nan
        return df_data

    def fetch_single_metadata(self, station) -> pd.DataFrame:      
        """
        For a single noaa site_id fetch the associated metadata.

        Parameters:
             A valid station id <str>
        Returns:
             dataframe of preselected metadata for a single station in the (keys,values) orientation

             This orientation facilitates aggregation upstream. Upstream will transpose this eventually
             to our preferred orientation with stations as index
        """
        domain='https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi/stations' # A fixed url for getting metadata

        meta=dict() 
        try:
            response = requests.get(f'{domain}/{station}.json')
            metares=response.text
            json_meta=json.loads(metares)
            meta['LAT'] = json_meta['stations'][0]['lat'] 
            meta['LAT'] = json_meta['stations'][0]['lat'] if json_meta['stations'][0]['lat'] !='' else np.nan
            meta['LON'] = json_meta['stations'][0]['lng'] if json_meta['stations'][0]['lng'] !='' else np.nan
            meta['NAME'] = json_meta['stations'][0]['name'] if json_meta['stations'][0]['name'] !='' else np.nan
            meta['LAT'] = json_meta['stations'][0]['lat'] if json_meta['stations'][0]['lat'] !='' else np.nan
            meta['UNITS'] = self._data_unit 
            meta['TZ'] = GLOBAL_TIMEZONE # data['utc_offset']
            meta['OWNER'] = 'NOAA/NOS'
            meta['STATE'] = json_meta['stations'][0]['state'] if json_meta['stations'][0]['state'] !='' else np.nan
            meta['COUNTY'] = np.nan
            #
            df_meta=pd.DataFrame.from_dict(meta, orient='index')
            df_meta.columns = [str(station)]
        except Exception as e:
            utilities.log.exception(f'NOAA WEB response meta error: {station} {response.text}: error is {e}')
            #sys.exit(1)
        return df_meta

#####################################################################################
##
## NOAA/NDBC source Realtime subclass
##

#class ndbc_fetch_data(fetch_station_data):
#    """
#    Invoke the NDBC subclass
#
#        This class develops the realtime data
#        Realtime data goes back now()-45days. 
#
#    Parameters:
#        station_id_list: list of NDBC buoy ids <str>
#        a tuple of (time_start, time_end) <str>,<str> format %Y-%m-%d %H:%M:%S
#        a valid PRODUCT id <str>: wave_height,pressure, wind_speed
#
#        One dict is used to manage jobs. The products dict maps generic product names used by high level codes
#        (keys) to the specific product names in NDBC 
#
#        UNITS: Based on an examinination of the data and comparison to plots on the NDBC website, it appears that
#        at least for the tested stations:
#            wave_height: meters
#            pressure: mbars 
#            wind_speed: m/s
# 
#        Currently tested input products:
#            wave_heigh
#            pressure
#            wind_speed
#    """
#    # dict( persistant tag: source specific tag )
#    # products defines current products (as keys) and uses the value as a column header in the returned data set
#
#    # NOTE: This dict maps the generic input data type (key) to the actual product name used by noaa-coops
#
#    products={ 'wave_height':'WVHT', # m
#               'air_pressure':'PRES', # hPa=mb
#               'wind_speed':'WSPD'} # mps
#
#    def __init__(self, station_id_list, periods, product='wave_height', units='metric',
#                resample_mins=15):
#        """
#        NDBC: National Data Buoy Center
#        Read buoy product data.
#
#        Parameters:
#            station_id_list: List of triplets [(id,name,state)...]
#            periods: A tuple of time range (timein,timeend).format %Y-%m-%d %H:%M:%S
#            product: <str> (Default='wave_height')  The generic product name
#            resample_mins: <int> time sampling. Specify 0 to get maximum resolution
#        """
#        self._data_unit=map_product_to_harvester_units(product)
#        try:
#            self._product=self.products[product] # self.products[product] # product
#            utilities.log.info(f'NDBC Fetching product {self._product}')
#        except KeyError:
#            utilities.log.error(f'NDBC No such product key. Input {product}, Available {self.products.keys()}')
#            raise
#            ##sys.exit(1)
#        else:
#            self._units='metric' # Redundant cleanup TODO
#        if units !='metric':
#            utilities.log.info(f'NDBC: units must be metric: {units}: Abort')
#            sys.exit(1)
#        super().__init__(station_id_list, periods, resample_mins=resample_mins)
#
#TODO metric con versions
#    def fetch_single_product(self, buoy, time_range) -> pd.DataFrame:
#        """
#        For a single NDBC site_id, process the tuple from the input period.
#        Aggregate them into a dataframe with index pd.timestamps and a single column
#        containing the desired product values. Rename the column to station id
#        
#        Parameters:
#            station <str>. A valid NDBC buoy id
#            time_range <tuple>. Start and end times (<str>,<str>) denoting time ranges
#
#        Returns:
#            dataframe of time (timestamps) vs values for the requested station
#        """
#        tstart,tend=time_range
#
#        utilities.log.info('NDBC: Iterate: start time is {}, end time is {}, buoy is {}'.format(tstart,tend,buoy[0]))
#        try:
#            df = NDBC.realtime_observations(buoy[0])
#            B = bp.realtime(buoy[0])
#            df=B.txt()
#            df.index.name='TIME'
#            df_data = df[self._product].to_frame()
#            df_data.columns=[str(buoy[0])]
#            df_data.sort_index(inplace=True) # From lowest to highest
#            df_data = df_data.loc[time_range[0]:time_range[1]]
#            df_data.replace(to_replace=99.0, value=np.nan, inplace=True)
#        except ConnectionError as ec:
#            utilities.log.error(f'Hard fail: Could not connect to NDBC for products {bouy[0]}: {ec}')
#        except HTTPError as eh:
#            utilities.log.error(f'Hard fail: HTTP error to NDBC for products: {eh}')
#        except Timeout:
#            utilities.log.error('Hard fail NDBC: Timeout')
#        except Exception as e:
#            utilities.log.error(f'NDBC data error: {e} was {self._product}')
#        try:
#            df_data=df_data.astype(float)
#        except Exception as e:
#            utilities.log.error(f'NDBC concat error: {e}')
#            df_data = np.nan
#        return df_data
#
#    def fetch_single_metadata(self, buoy) -> pd.DataFrame:
#        """
#        For a single NDBC site_id fetch the associated metadata.
#        The choice of metadata is highly subjective at this time.
#
#        Parameters:
#             A valid buoy id <str>
#        Returns:
#             dataframe of preselected metadata for a single station in the (keys,values) orientation
#
#             This orientation facilitates aggregation upstream. Upstream will transpose this eventually
#             to our preferred orientation with stations as index
#        """
#        meta=dict()
#        df_latest = NDBC.latest_observations().set_index('station')
#        lat, lon = df_latest.loc[buoy[0]][['latitude','longitude']]
#        meta['LAT'] = lat
#        meta['LON'] = lon
#        meta['NAME'] =  buoy[1]
#        meta['UNITS'] = self._data_unit 
#        meta['TZ'] = GLOBAL_TIMEZONE
#        meta['OWNER'] = 'NONE'
#        meta['STATE'] = buoy[2]
#        meta['COUNTY'] = np.nan # None
#        df_meta=pd.DataFrame.from_dict(meta, orient='index')
#        df_meta.columns = [str(buoy[0])]
#        return df_meta
#
##
## NOAA/NDBC source historical subclass
##

#class ndbc_fetch_historic_data(fetch_station_data):
#    """
#    Invoke the NDBC historical subclass
#        This class develops the historical data
#        Historical data will not return data from the current year.
#
#    Parameters:
#        station_id_list: list of NDBC buoy ids <str>
#        a tuple of (time_start, time_end) <str>,<str> format %Y-%m-%d %H:%M:%S
#        a valid PRODUCT id <str>: wave_height,pressure, wind_speed
#
#        One dict is used to manage jobs. The products dict maps generic product names used by high level codes
#        (keys) to the specific product names in NDBC 
#
#        UNITS: Based on an examinination of the data and comparison to plots on the NDBC website, it appears that
#        at least for the tested stations:
#            wave_height: meters
#            pressure: mbars 
#            wind_speed: m/s
# 
#        Currently tested input products:
#            wave_height
#            pressure
#            wind_speed
#    """
#    # dict( persistant tag: source specific tag )
#    # products defines current products (as keys) and uses the value as a column header in the returned data set
#
#    # NOTE: This dict maps the generic input data type (key) to the actual product name used by noaa-coops
#
#    products={ 'wave_height':'WVHT', # m
#               'air_pressure':'PRES', # hPa=mb
#               'wind_speed':'WSPD'} # mps
#
#    def __init__(self, station_id_list, periods, product='wave_height', units='metric',
#                resample_mins=15):
#        """
#        NDBC: National Data Buoy Center
#        Read Historical buoy product data.
#
#        Parameters
#            station_id_list: :List of triplets [(id,name,state)...]
#            periods: A tuple of time range (timein,timeend).format %Y-%m-%d %H:%M:%S
#            product: <str> (Default='wave_height')  The generic product name
#            resample_mins: <int> time sampling. Specify 0 to get maximum resolution
#        """
#        self._data_unit=map_product_to_harvester_units(product)
#        try:
#            self._product=self.products[product] # self.products[product] # product
#            utilities.log.info(f'NDBC Fetching Historicalproduct {self._product}')
#        except KeyError:
#            utilities.log.error(f'NDBC No such historical product key. Input {product}, Available {self.products.keys()}')
#            raise
#            ##sys.exit(1)
#        else:
#            self._units='metric' # Redundant cleanup TODO
#        if units !='metric':
#            utilities.log.info('NDBC: units must be metric: {}: Abort'.format(units))
#            sys.exit(1)
#        super().__init__(station_id_list, periods, resample_mins=resample_mins)
#
#    def get_year_list_from_timerange(self, time_range):
#        """
#        The input time_range tuple is queried to determine what year we are interested in.
#        No checks are made to ensure the CURRENT year is excluded. If it is included,
#        then the subsequent historical data call will fail
#
#        Parameters:
#            input time tuple: (<str>,<str>) formats are '%Y-%m-%d %H:%M:%S'
#        Returns:
#            year_list: list sorted range of years
#        """
#        dformat = '%Y-%m-%d %H:%M:%S'
#        yin = dt.datetime.strptime(time_range[0], dformat).year
#        yout = dt.datetime.strptime(time_range[1], dformat).year
#        if yin > yout:
#            yin,yout = yout,yim
#        year_list=list(range(yin,yout+1))
#        return year_list
#
### Implement the required two methods
#
#    def fetch_single_product(self, buoy, time_range) -> pd.DataFrame:
#        """
#        For a single NDBC site_id, process the tuple from the input period.
#        Aggregate them into a dataframe with index pd.timestamps and a single column
#        containing the desired product values. Rename the column to station id
#   
#        As of this writing, it is unclear if the time_range argument to buoypy 
#        works. So I build this year_list approach to be sure
#        
#        Parameters:
#            station <str>. A valid NDBC buoy id
#            time_range <tuple>. Start and end times (<str>,<str>) denoting time ranges
#
#        Returns:
#            dataframe of time (timestamps) vs values for the requested station
#        """
#        tstart,tend=time_range
#
#        utilities.log.info('NDBC_HISTORIC: Iterate: start time is {}, end time is {}, buoy is {}'.format(tstart,tend,buoy[0]))
#
#        data_list=list()
#        year_list = self.get_year_list_from_timerange(time_range)
#        for year in year_list:
#            print(f'NDBC_HISTORICAL: Found year {year}')
#            try:
#                H = bp.historic_data(buoy[0],year)
#                df = H.get_stand_meteo() 
#                df.index.name='TIME'
#                df_data = df[self._product].to_frame()
#                df_data.columns=[str(buoy[0])]
#                df_data.sort_index(inplace=True) # From lowest to highest
#                df_data.replace(to_replace=99.0, value=np.nan, inplace=True)
#                data_list.append(df_data)
#            except ConnectionError as ec:
#                utilities.log.error(f'Hard fail: Could not connect to NDBC for products {buoy[0]}: {ec}')
#            except HTTPError:
#                utilities.log.error('Hard fail: HTTP error to NDBC for products')
#            except Timeout:
#                utilities.log.error('Hard fail NDBC: Timeout')
#            except Exception as e:
#                utilities.log.error(f'NDBC data error: {e} was {self._product}')
#            df_data = pd.concat(data_list, axis=0) 
#        try:
#            df_data=df_data.astype(float)
#        except Exception as e:
#            utilities.log.error(f'NDBC_HISTORIC float assignment error: {e}')
#            df_data = np.nan
#        df_data = df_data.loc[time_range[0]:time_range[1]]
#        return df_data
#
#    def fetch_single_metadata(self, buoy) -> pd.DataFrame:
#        """
#        For a single NDBC site_id fetch the associated metadata.
#        The choice of data is highly subjective at this time.
#
#        Parameters:
#             A valid buoy id <str>
#        Returns:
#             dataframe of preselected metadata for a single station in the (keys,values) orientation
#
#             This orientation facilitates aggregation upstream. Upstream will transpose this eventually
#             to our preferred orientation with stations as index
#        """ 
#        try:
#            meta=dict()
#            df_latest = NDBC.latest_observations().set_index('station')
#            df_latest.to_csv('dump.csv')
#            lat, lon = df_latest.loc[buoy[0]][['latitude','longitude']]
#            meta['LAT'] = lat
#            meta['LON'] = lon
#            meta['NAME'] =  buoy[1]
#            meta['UNITS'] = self._data_unit 
#            meta['TZ'] = GLOBAL_TIMEZONE
#            meta['OWNER'] = 'NONE'
#            meta['STATE'] = buoy[2]
#            meta['COUNTY'] = np.nan # None
#            df_meta=pd.DataFrame.from_dict(meta, orient='index')
#            df_meta.columns = [str(buoy[0])]
#        except KeyError:
#             pass
#             utilities.log.warning(f'Station {buoy[0]} had no metadata: skip it')
#        except Exception as e:
#            utilities.log.exception(f'NDBC Historical meta error: {e}')
#            raise
#            ##sys.exit(1)
#        return df_meta
